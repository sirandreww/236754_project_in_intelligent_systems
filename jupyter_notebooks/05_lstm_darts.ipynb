{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df56602b-a665-458b-a581-dd1faa97b5e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Long Short-Term Memory (LSTM) Using Darts\n",
    "\n",
    "## What is Darts? \n",
    "\n",
    "Darts is a python time series forcasting library. We wanted to see what it can do with its implementation of lstm. How much does it differ from ours? Is it better or worse? what about trainning time? It is generally better to use a library then implement everything by hand. We will be using darts for the upcoming time series forcasting models.\n",
    "\n",
    "Using darts enables us to test different algorithms faster and to tune parameters faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63bdb1f-e62d-4f3f-90fe-6345d8004f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.darts__lstm as lstm\n",
    "import src.framework__test_bench as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b937e4f-bf66-4243-aa23-d3f649305cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST BENCH] Powering on test bench\n",
      "[TEST BENCH] testing metric='container_mem', app='keepalived'.\n",
      "[TEST BENCH] Fetching data for metric='container_mem', app='keepalived'.\n",
      "[TEST BENCH] Subsampling data from 1 sample per 1 minute to 1 sample per 30 minutes.\n",
      "[TEST BENCH] Throwing out data that is less than 15.0 hours long.\n",
      "[TEST BENCH] Scaling data.\n",
      "[TEST BENCH] Splitting data into train and test\n",
      "[TEST BENCH] Amount of train/test data is 435\n",
      "[TEST BENCH] Making an instance of the class we want to test\n",
      "[TEST BENCH] Starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 02:34:30 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 73515 samples.\n",
      "C:\\Users\\Andrew\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.11171057976191785 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "2022-07-18 02:34:30 darts.models.forecasting.torch_forecasting_model INFO: Time series values are 32-bits; casting model to float32.\n",
      "2022-07-18 02:34:30 pytorch_lightning.utilities.rank_zero INFO: GPU available: False, used: False\n",
      "2022-07-18 02:34:30 pytorch_lightning.utilities.rank_zero INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-07-18 02:34:30 pytorch_lightning.utilities.rank_zero INFO: IPU available: False, using: 0 IPUs\n",
      "2022-07-18 02:34:30 pytorch_lightning.utilities.rank_zero INFO: HPU available: False, using: 0 HPUs\n",
      "2022-07-18 02:34:30 pytorch_lightning.callbacks.model_summary INFO: \n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 15.1 K\n",
      "4 | V             | Linear           | 61    \n",
      "---------------------------------------------------\n",
      "15.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.2 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6218eb3a4a64b72a7415d5a016b21df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tb = tb.TestBench(\n",
    "    class_to_test=lstm.DartsLSTMTester,\n",
    "    path_to_data=\"./data/\",\n",
    "    tests_to_perform=[{\"metric\": \"container_mem\", \"app\": \"keepalived\", \"prediction length\": 16, \"sub sample rate\": 30, \"data length limit\": 30},]\n",
    ")\n",
    "tb.run_training_and_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa142111-b387-4e0b-83ee-81de093148ad",
   "metadata": {},
   "source": [
    "Let's print information about the hardware we're running on, this is so we make sure that training time comparisons are fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57599138-5fb0-4ef1-a9c5-bdb984df44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc45a56-c48f-40ae-80b9-dca754fca171",
   "metadata": {},
   "source": [
    "Now we can make a table that summarizes the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d295bee-4454-4664-a129-13e6b7c8d9cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "| metric   | app         | training time | mse | precision | recall | F1  | MASE | MAPE |\n",
    "| :-       | :-          | :-:           | :-: | :-:       | :-:    | :-: | :-:  | :-:  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
